[
    {
      "recommended": true,
      "name": "LLaMA 2 Chat",
      "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
      "filename": "llama-2-7b-chat.ggmlv3.q2_K.bin",
      "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q2_K.bin",
      "parameterCount": "7B",
      "quantization": "2-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "recommended": true,
      "name": "LLaMA 2 Chat",
      "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
      "filename": "llama-2-7b-chat.ggmlv3.q4_K_M.bin",
      "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q4_K_M.bin",
      "parameterCount": "7B",
      "quantization": "4-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "recommended": true,
      "name": "LLaMA 2 Chat",
      "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
      "filename": "llama-2-7b-chat.ggmlv3.q8_0.bin",
      "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin",
      "parameterCount": "7B",
      "quantization": "8-bit",
      "labels": []
    },
    {
      "name": "LLaMA 2 Chat",
      "description": "New k-quant method. Uses GGML_TYPE_Q8_K for all tensors - 6-bit quantization",
      "filename": "llama-2-13b-chat.ggmlv3.q6_K.bin",
      "url": "https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q6_K.bin",
      "parameterCount": "13B",
      "quantization": "6-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "LLaMA",
      "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
      "filename": "llama-7b.ggmlv3.q2_K.bin",
      "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q2_K.bin",
      "parameterCount": "7B",
      "quantization": "2-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "LLaMA",
      "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
      "filename": "llama-7b.ggmlv3.q4_K_M.bin",
      "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q4_K_M.bin",
      "parameterCount": "7B",
      "quantization": "4-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "LLaMA",
      "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
      "filename": "llama-7b.ggmlv3.q8_0.bin",
      "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q8_0.bin",
      "parameterCount": "7B",
      "quantization": "8-bit",
      "labels": []
    },
    {
      "name": "LLaMA",
      "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
      "filename": "llama-13b.ggmlv3.q2_K.bin",
      "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q2_K.bin",
      "parameterCount": "13B",
      "quantization": "2-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "LLaMA",
      "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
      "filename": "llama-13b.ggmlv3.q4_K_M.bin",
      "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q4_K_M.bin",
      "parameterCount": "13B",
      "quantization": "4-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "LLaMA 13B",
      "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
      "filename": "llama-13b.ggmlv3.q8_0.bin",
      "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q8_0.bin",
      "parameterCount": "13B",
      "quantization": "8-bit",
      "labels": []
    },
    {
      "name": "Vicuna",
      "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
      "filename": "vicuna-7b-v1.3.ggmlv3.q2_K.bin",
      "url": "https://huggingface.co/localmodels/Vicuna-7B-v1.3-ggml/resolve/main/vicuna-7b-v1.3.ggmlv3.q2_K.bin",
      "parameterCount": "7B",
      "quantization": "2-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "Vicuna",
      "description": "Original llama.cpp quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.",
      "filename": "vicuna-7b-v1.3.ggmlv3.q4_1.bin",
      "url": "https://huggingface.co/localmodels/Vicuna-7B-v1.3-ggml/resolve/main/vicuna-7b-v1.3.ggmlv3.q4_1.bin",
      "parameterCount": "7B",
      "quantization": "4-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "Vicuna",
      "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
      "filename": "vicuna-7b-v1.3.ggmlv3.q8_0.bin",
      "url": "https://huggingface.co/localmodels/Vicuna-7B-v1.3-ggml/resolve/main/vicuna-7b-v1.3.ggmlv3.q8_0.bin",
      "parameterCount": "7B",
      "quantization": "8-bit",
      "labels": []
    },
    {
      "name": "Vicuna",
      "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
      "filename": "vicuna-33b.ggmlv3.q2_K.bin",
      "url": "https://huggingface.co/localmodels/Vicuna-33B-v1.3-ggml/resolve/main/vicuna-33b.ggmlv3.q2_K.bin",
      "parameterCount": "33B",
      "quantization": "2-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "Vicuna",
      "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
      "filename": "vicuna-33b.ggmlv3.q8_0.bin",
      "url": "https://huggingface.co/localmodels/Vicuna-33B-v1.3-ggml/resolve/main/vicuna-33b.ggmlv3.q8_0.bin",
      "parameterCount": "33B",
      "quantization": "8-bit",
      "labels": []
    },
    {
      "name": "WizardLM Uncensored",
      "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
      "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q2_K.bin",
      "url": "https://huggingface.co/localmodels/WizardLM-7B-v1.0-Uncensored-ggml/resolve/main/wizardlm-7b-v1.0-uncensored.ggmlv3.q2_K.bin",
      "parameterCount": "7B",
      "quantization": "2-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "WizardLM",
      "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
      "filename": "wizardlm-13b-v1.1.ggmlv3.q4_K_M.bin",
      "url": "https://huggingface.co/localmodels/WizardLM-13B-v1.1-ggml/resolve/main/wizardlm-13b-v1.1.ggmlv3.q4_K_M.bin",
      "parameterCount": "13B",
      "quantization": "4-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "WizardLM",
      "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
      "filename": "wizardlm-13b-v1.1.ggmlv3.q8_0.bin",
      "url": "https://huggingface.co/localmodels/WizardLM-13B-v1.1-ggml/resolve/main/wizardlm-13b-v1.1.ggmlv3.q8_0.bin",
      "parameterCount": "13B",
      "quantization": "8-bit",
      "labels": []
    },
    {
      "name": "WizardLM 30B",
      "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
      "filename": "wizardlm-30b.ggmlv3.q2_K.bin",
      "url": "https://huggingface.co/localmodels/WizardLM-30B-v1.0-ggml/resolve/main/wizardlm-30b.ggmlv3.q2_K.bin",
      "parameterCount": "30B",
      "quantization": "2-bit",
      "labels": [
        "k-quant"
      ]
    },
    {
      "name": "WizardLM 30B (8-bit)",
      "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
      "filename": "wizardlm-30b.ggmlv3.q8_0.bin",
      "url": "https://huggingface.co/localmodels/WizardLM-30B-v1.0-ggml/resolve/main/wizardlm-30b.ggmlv3.q8_0.bin",
      "parameterCount": "30B",
      "quantization": "8-bit",
      "labels": []
    }
  ]